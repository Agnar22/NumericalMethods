{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1\n",
    "### 8/10 perfect solutions required for full score\n",
    "\n",
    "These exercises should be done on a computer using Python. \tPlease submit a Jupyter-notebook, containing both your codes and the report in the same file. Provide properly commented codes.\n",
    "\n",
    "Some of the tasks involve sparse matrices, to obtain efficient code it is preferable to work with sparse matrices using for example the library *sparse* of  *scipy*. In these exercises you are allowed to work with full matrices if you find it easier, but make sure that the comparisons of the various methods in your implementations, e.g. when comparing running times, are fair. Be explicit about these issues in your report.\n",
    "\n",
    "**You will need an additional Python library for this exerise:** The library `autograd` will be used to compute the numerical Jacobian. It is not recommended to install this into the default annaconda environment, but instead create a seperate envirome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import decimal as dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Propagation of rounding errors\n",
    "The following exercise illustrates the effects of propagation of rounding errors in a numerical algorithm.\n",
    "\n",
    " \n",
    "We want to make a programme to approximate the derivative of a differentiable function $f:\\mathbb{R} \\rightarrow \\mathbb{R}$. We use the simple approximation\n",
    "\n",
    "$$f'(x)=\\frac{f(x+\\Delta x)-f(x)}{\\Delta x}+\\mathcal{O}(\\Delta x),\\qquad f'(x)\\approx \\frac{f(x+\\Delta x)-f(x)}{\\Delta x}.$$\n",
    "\n",
    "1. Use a Taylor expansion to prove that the error given by this approximation is indeed $\\mathcal{O}(\\Delta x)$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answer here ...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now take $f(x)=\\exp(x)$, and use the formula above to approximate \n",
    "\n",
    "$$\\left.\\frac{d}{dx}\\exp (x)\\right|_{x=\\frac{\\pi}{4}}.$$\n",
    "\n",
    "2. Make a simple Python programme to compute the absolute error committed by the approximation for decreasing values of $\\Delta x$, e.g $\\Delta x=\\frac{1}{2^k}$ for $k=1,2,\\dots , N$, choosing a value of $N$ between $30$ and $40$.\n",
    "Plot the results (values of $\\Delta x$ versus the corresponding absolute error) in a logarithmic scale.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see in your plot that for very small values of $\\Delta x$ the error starts to increase, compromising the convergence of the approximation method. This is due to the propagation of rounding error. The results improve if we can compute our formulae with a higher precision.\n",
    "\n",
    "3. In this exercise you should learn how to use the library `decimal` in Python in order to perform the calculations of the previous experiment with higher precision. \n",
    "Consider the same parameters used in the previous experiment ($\\Delta x=\\frac{1}{2^k}$ for $k=1,2,\\dots , N$ and same $N$ as above), increase the level of precision (look up `getcontext().prec` in the `decimal` library) and produce a plot which does not show the effects of propagation of rounding errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Newton method\n",
    "\n",
    "In this exercise, we will make use of the `autograd` package. As we have done at the start of this noteobook, when using this package, you must `import` Numpy using `autograd.numpy`. We use the `jacobian` function. Given a function $F:\\mathbb{R}^n \\to \\mathbb{R}^n$, this outputs the function $\\mathbb{R}^n \\to \\mathbb{R}^n, x\\mapsto J x$, where $J$ is the Jacobian of $F$. See the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import jacobian\n",
    "\n",
    "def F(x):\n",
    "    return np.array([x[0] + x[1], x[1]*x[0]])\n",
    "\n",
    "# Computing the numerical Jacobian\n",
    "DF = jacobian(F)\n",
    "# Testing our function\n",
    "x = np.array([3.0, 1.0])\n",
    "F(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. You are given the following two functions:\n",
    "$$\n",
    "    F_1(x_1, x_2) = (x_1 - x_2 - 1, x_1^2 + x_2^2 - 4)\n",
    "$$\n",
    "$$\n",
    "    F_2(x_1, x_2, x_3) = (4x_1^2 - x_2^2, 4x_1x_2^2 - 1, 3x_3 - 1)\n",
    "$$\n",
    "  - Using the package `autograd` for automatic differentiation to compute Jacobians, create a function `newton` which implements the Newton method for systems of nonlinear equations $F(\\mathbf{x})=0$.\n",
    "  - Apply the method to $F_1$ and $F_2$. These numerical experiments should give evidence that the Newton method converges for these two problems.\n",
    "  - Choose an initial value close enough in norm to one solution of the system, and compute the norm of the difference between two consecutive iterates $\\mathbf{x}^k$ and $\\mathbf{x}^{k+1}$ to provide numerical evidence that your method converges. As an alternative way of measuring convergence consider the norm of $F(\\mathbf{x}_k)$. Use the $\\infty$-norm. Plot the values of $\\|\\mathbf{x}^{k+1}-\\mathbf{x}^k\\|_{\\infty}$ and $\\|F(\\mathbf{x}_k)\\|_{\\infty}$ for each iteration  $k=0,1,\\dots, K-1$ where $K$ is the total number of iterations ($K$ is a number that you choose). Use a semi-logarithmic scale on the y-axis (semilog plot). Your answer is a figure with a few comments in text about what the picture shows and what is the problem you are solving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Newton method implementation\n",
    "def newton(F, x0, x, maxIter):\n",
    "    '''\n",
    "    Implements the Newton root finding method for systems of equations.\n",
    "    \n",
    "    --Parameters--\n",
    "    F: function \n",
    "        N-array to N-array function\n",
    "    x0: N-array\n",
    "        Initial guess\n",
    "    x: N-array\n",
    "        actual solution\n",
    "    maxIter: int\n",
    "        Maximum number of iterations\n",
    "    \n",
    "    --Outputs--\n",
    "    y: N-array\n",
    "        Approximation to solution of F(y)=0\n",
    "    '''\n",
    "    pass\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the method to your two functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use the implemented Newton method and implement a stopping criterion.\n",
    "As stopping criterion, you should use a maximum number of iterations as well as a tolerance $TOL$ such that the iteration is stopped whenever $\\|F(\\mathbf{x}^k)\\|_{\\infty}\\le TOL$ and $\\|\\mathbf{x}^{k+1}-\\mathbf{x}^k\\|_{\\infty}\\le TOL$. \n",
    "Provide numerical evidence that your code works as it should by  printing the values $\\|F(\\mathbf{x}^k)\\|_{\\infty}$ for each iteration , and making a plot of $\\|F(\\mathbf{x}^k)\\|_{\\infty}$, $\\|\\mathbf{x}^{k+1}-\\mathbf{x}^k\\|_{\\infty}$, for all $k$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement your method with a stopping criterion\n",
    "def newton(F, x0, x, maxIter):\n",
    "    '''\n",
    "    Implements the Newton root finding method for systems of equations.\n",
    "    \n",
    "    --Parameters--\n",
    "    F: function \n",
    "        N-array to N-array function\n",
    "    x0: N-array\n",
    "        Initial guess\n",
    "    x: N-array\n",
    "        actual solution\n",
    "    maxIter: int\n",
    "        Maximum number of iterations\n",
    "    TOL: float\n",
    "        Tolerance for error\n",
    "    \n",
    "    --Outputs--\n",
    "    y: N-array\n",
    "        Approximation to solution of F(y)=0\n",
    "    '''\n",
    "    pass\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. This question is open-ended meaning that there are many different ways to approach the question. \n",
    "Think of a good way of verifying numerically that the converegence of the Newton iteration is quadratic. \n",
    "\n",
    "**Hint**. You should consider a problem for which you know the solution or alternatively generate a reference solution (i.e. an approximation with very high accuracy). Use a converging iterative method with a very large number of iterations to obtain your reference solution. Use then the obtained reference solution in place of the exact solution for checking that the convergence indeed is *quadratic* or at *least quadratic*. Use the definition of quadratic convrgence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Linear systems\n",
    "\n",
    "Consider the linear system \n",
    "\t\\begin{equation*}\n",
    "\tA \\mathbf{u} = \\mathbf{f},\n",
    "\t\\end{equation*}\n",
    "where $A$ is a finite-difference discretization of the 2D Laplacian, this is a block tridiagonal matrix, implemented in the code below, and $\\mathbf{f}$ is the vector with all components equal to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from scipy import linalg\n",
    "\n",
    "# Creating the 2D laplacian matrix.\n",
    "nx, ny = 5, 5\n",
    "N  = nx*ny\n",
    "main_diag = np.ones(N)*-4.0\n",
    "side_diag = np.ones(N-1)\n",
    "side_diag[np.arange(1,N)%4==0] = 0\n",
    "up_down_diag = np.ones(N-3)\n",
    "diagonals = [main_diag,side_diag,side_diag,up_down_diag,up_down_diag]\n",
    "laplacian = sparse.diags(diagonals, [0, -1, 1,nx,-nx])\n",
    "print(laplacian*np.eye(N))\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we introduce the following notation:\n",
    "\n",
    "- $A_d$ the diagonal part of $A$,\n",
    "- $A_l$ is **minus** the strictly lower-triangular part of $A$,\n",
    "- $A_u$ is **minus** the strictly upper-triangular part of $A$,\n",
    "\n",
    "with this notation, $A = A_d - A_l - A_u$.\n",
    "\n",
    "We will now approximate the solution of this linear system using three different iterative methods of the type \n",
    "\t\\begin{equation*}\n",
    "\t\tM \\mathbf{u}^{(k+1)} = N \\mathbf{u}^{(k)}+\\mathbf{f},\n",
    "\t\\end{equation*}\n",
    "\twhere $A = M - N$ with $|M|\\ne0$. These methods are aslo known as splitting methods and  each choice of $M$ (and $N=M-A$) determines a different method. We will use the following three iterative methods to calculate approximate solutions to $\\mathbf{u}$:\n",
    "\n",
    "a) Jacobi ($M = A_d$).\n",
    "\n",
    "b) Forward Gauss-Seidel ($M = A_d-A_l$).\n",
    "\n",
    "c) Successive over relaxation ($M = A_d-\\omega A_l$, where you can choose the value of $\\omega$).\n",
    "\n",
    "1. Create a function that implements the above method. This function should take as input the matrix `A` and `M`, and outputs the solution to the linear system `u`. Use the three methods mentioned above to find the solution to the linear system with `nx=ny=10` and `f` equal to the one vector. Confirm that what you obtain is indeed a solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code - Create the function\n",
    "def solveSystem(A, M, f, u0):\n",
    "    '''\n",
    "        Solves the system Au=f. \n",
    "        Takes as input the matrix A and function f, M which defines the method,\n",
    "    and an initial guess u0.\n",
    "        Outputs the solution u.\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your function to solve the problem with each of the methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Consider the residual vector $\\mathbf{r}^k:=\\mathbf{f}-A\\mathbf{u}^k$. We wish to compare the speed of convergence for each of the methods above. \n",
    "   - Produce a semi-log plot of the  $2$-norm of the relative residual  $\\frac{\\|\\mathbf{r}^k\\|_2}{\\|\\mathbf{r}^0\\|_2}$ versus the number of iterations. Plot all the methods in the same plot for comparison. \n",
    "   - Try choosing different values of $\\omega$ in the SOR method, and try to find a value that gives you optimal convergence for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Implement a stopping criterion for the three methods. Then measure the relative time each method takes per iteration, and the time needed to guarantee that the norm of the relative residual is below tolerances $TOL=1e-7$ and $TOL=1e-14$. Use the function *time.time()* in Python.\n",
    "Make a table containing the results for each method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import spsolve # Solver, similiar to the normal linalg.solve for sparse matrices\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Your Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make your table here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. This exercise is open-ended. Calculate the spectral radius of $A_1^{-1}A_2$ and see how it relates to the convergence of the method (Hint: Python's `numpy.linalg.eigvals(A)` function returns the eigenvalues of $A$ ). Recall that there is a theorem that links the spectral radius of $A_1^{-1}A_2$ to the speed of convergence of the iteration. The theorem states that a spectral radius below $1$ is a necessary and sufficient condition for convergence independently on how $\\mathbf{u}^0$ is chosen. In fact, the smaller the spectral radius the faster the convergence. Does this theorem reflect what happens in your example? How can you verify this with numerical experiments? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
